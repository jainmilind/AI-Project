{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RULES class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RULES:\n",
    "    def __init__(self, contains_header=True, discretize_mode='equal', number_bins=7, discretize_ints=False):\n",
    "        self.contains_header = contains_header\n",
    "        self.discretize_mode = discretize_mode\n",
    "        self.number_bins = number_bins\n",
    "        self.discretize_ints = discretize_ints\n",
    "        self.bins = []\n",
    "\n",
    "        self.attribute_names = None\n",
    "        self.preproc_dict = None\n",
    "        self.labels_dict = None\n",
    "\n",
    "        self.rules = []\n",
    "        self.most_probable_class = None\n",
    "        self.n_attributes = 0\n",
    "\n",
    "    # ######################### F I T #########################\n",
    "    def fit(self, x, y, method='RRULES', show_rules=True, show_metrics=True, show_time=True):\n",
    "        since = time.time()\n",
    "        x, y = self.__preproc_train_data(x, y)\n",
    "        preproc_time = time.time() - since\n",
    "\n",
    "        since = time.time()\n",
    "        if method == 'RRULES':\n",
    "            print('Training with RRULES...')\n",
    "            self.__fit_RRULES(x, y)\n",
    "        elif method == 'Original':\n",
    "            print('Training with Original RULES...')\n",
    "            self.__fit_original_RULES(x, y)\n",
    "        fit_time = time.time() - since\n",
    "\n",
    "        since = time.time()\n",
    "        if show_rules:\n",
    "            metrics = None\n",
    "            if show_metrics:\n",
    "                metrics = self.__compute_metrics(x, y)\n",
    "                metrics_time = time.time() - since\n",
    "                since = time.time()\n",
    "            print('\\nInduced rules:')\n",
    "            print(self.__print_rules(metrics))\n",
    "            print()\n",
    "        print_time = time.time() - since\n",
    "\n",
    "        if show_time:\n",
    "            print(f\"Time to preprocess data = {preproc_time:.2f}s\")\n",
    "            print(f\"Time to fit data = {fit_time:.2f}s\")\n",
    "            if show_rules:\n",
    "                if show_metrics:\n",
    "                    print(f\"Time to comput metrics = {metrics_time:.2f}s\")\n",
    "                print(f\"Time to print rules = {print_time:.2f}s\")\n",
    "            print()\n",
    "\n",
    "    def __fit_RRULES(self, x, y):\n",
    "        # We calculate the most probable class to create a default rule for unseen combinations of attributes\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        self.most_probable_class = classes[np.argmax(counts)]\n",
    "\n",
    "        # ##### RRULES #####\n",
    "        n_examples, n_attributes = x.shape\n",
    "        self.n_attributes = n_attributes\n",
    "        # Track non-classified by index\n",
    "        indices_not_classified = np.arange(n_examples)\n",
    "\n",
    "        # For each n_conditions = 1, ..., n_attributes\n",
    "        for n_conditions in range(1, n_attributes + 1):\n",
    "            # Generate all possible combinations of attributes (without repetition and without order)\n",
    "            # of length n_conditions\n",
    "            attribute_combinations_n = combinations(range(n_attributes), n_conditions)\n",
    "            # For each combination of attributes (columns)\n",
    "            for attribute_group in attribute_combinations_n:\n",
    "                lists_of_values = []\n",
    "                # Calculate the unique values of the chosen attributes from the non-classified instances,\n",
    "                # and generate all combinations of selectors <attribute, value> given the chosen attributes\n",
    "                # These combination of selectors form conditions\n",
    "                for attribute in attribute_group:\n",
    "                    lists_of_values.append(np.unique(x[indices_not_classified, attribute]))\n",
    "                value_combinations = product(*lists_of_values)\n",
    "                # For each condition <att1, val1>, <att2, val2>, ...\n",
    "                for value_group in value_combinations:\n",
    "                    # Find indices of ALL INSTANCES that match the condition\n",
    "                    indices_match = np.where((x[:, list(attribute_group)] == value_group).all(axis=1))[0]\n",
    "                    # Find indices of NON-CLASSIFIED INSTANCES that match the condition\n",
    "                    indices_match_not_classified = \\\n",
    "                        np.where((x[np.ix_(indices_not_classified, attribute_group)] == value_group).all(axis=1))[0]\n",
    "                    if len(indices_match) == 0:\n",
    "                        # This condition is not present in the training set of examples\n",
    "                        continue\n",
    "                    if len(indices_match_not_classified) == 0:\n",
    "                        # Although this condition is present in the set of examples,\n",
    "                        # it does not match any non-classified instance\n",
    "                        # It is the case of a condition that could end generating an IRRELEVANT RULE\n",
    "                        continue\n",
    "                    # Take the ground truth of the matched instances and look if they belong to a single class\n",
    "                    classes = y[indices_match]\n",
    "                    unique_classes = np.unique(classes)\n",
    "                    if len(unique_classes) == 1:\n",
    "                        # Generate the rule and add it to the set of rules\n",
    "                        # The rule is encoded as the set of attributes to match, their values and the class\n",
    "                        self.rules.append((attribute_group, value_group, unique_classes[0]))\n",
    "                        # Remove the classified instances from the set of non-classified ones\n",
    "                        indices_not_classified = np.setdiff1d(indices_not_classified, indices_match, assume_unique=True)\n",
    "                        # If there aren't any more instances to classify, return\n",
    "                        if len(indices_not_classified) == 0:\n",
    "                            return\n",
    "                    # If we are in the last iteration, we are checking for the full antecedent\n",
    "                    # If there is more than a single class, we have a contradiction in the data\n",
    "                    # Let's choose the most probable class (or random if tie)\n",
    "                    elif n_conditions == n_attributes:\n",
    "                        print(\"WARNING: There are contradictions in the training set\")\n",
    "                        unique_classes, counts = np.unique(classes, return_counts=True)\n",
    "                        self.rules.append((attribute_group, value_group, unique_classes[np.argmax(counts)]))\n",
    "                        # Remove the classified instances from the set of non-classified ones\n",
    "                        indices_not_classified = np.setdiff1d(indices_not_classified, indices_match, assume_unique=True)\n",
    "                        # If there aren't any more instances to classify, return\n",
    "                        if len(indices_not_classified) == 0:\n",
    "                            return\n",
    "\n",
    "    def __fit_original_RULES(self, x, y):\n",
    "        # We calculate the most probable class to create a default rule for unseen combinations of attributes\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        self.most_probable_class = classes[np.argmax(counts)]\n",
    "\n",
    "        # ##### RULES #####\n",
    "        n_examples, n_attributes = x.shape\n",
    "        self.n_attributes = n_attributes\n",
    "        # Track non-classified by index\n",
    "        indices_not_classified = np.arange(n_examples)\n",
    "\n",
    "        # For each n_conditions = 1, ..., n_attributes\n",
    "        for n_conditions in range(1, n_attributes + 1):\n",
    "            # Check stopping condition only at the beginning of every outer iteration\n",
    "            if len(indices_not_classified) == 0:\n",
    "                return\n",
    "            # Generate all combinations of selectors of length n_conditions\n",
    "            conditions = []\n",
    "            # Generate all possible combinations of attributes (without repetition and without order)\n",
    "            # of length n_conditions\n",
    "            attribute_combinations_n = combinations(range(n_attributes), n_conditions)\n",
    "            # For each combination of attributes (columns)\n",
    "            for attribute_group in attribute_combinations_n:\n",
    "                lists_of_values = []\n",
    "                # Calculate the unique values of the chosen attributes, and generate\n",
    "                # all combinations of selectors <attribute, value> given the chosen attributes\n",
    "                # These combination of selectors form conditions\n",
    "                for attribute in attribute_group:\n",
    "                    lists_of_values.append(np.unique(x[indices_not_classified, attribute]))\n",
    "                value_combinations = product(*lists_of_values)\n",
    "                for value_group in value_combinations:\n",
    "                    conditions.append((attribute_group, value_group))\n",
    "\n",
    "            # For each condition <att1, val1>, <att2, val2>, ...\n",
    "            for attribute_group, value_group in conditions:\n",
    "                # Find indices of ALL INSTANCES that match the condition\n",
    "                indices_match = np.where((x[:, list(attribute_group)] == value_group).all(axis=1))[0]\n",
    "                if len(indices_match) == 0:\n",
    "                    # This condition is not present in the training set of examples\n",
    "                    continue\n",
    "                # Take the ground truth of the matched instances and look if they belong to a single class\n",
    "                classes = y[indices_match]\n",
    "                unique_classes = np.unique(classes)\n",
    "                if len(unique_classes) == 1:\n",
    "                    # Check for irrelevant conditions\n",
    "                    is_irrelevant = False\n",
    "                    if n_conditions > 1:\n",
    "                        for rule in self.rules:\n",
    "                            # If there is a previous rule (with less conditions) that includes all the selectors for\n",
    "                            # the new rule, this becomes irrelevant as it does not classify any new instance\n",
    "                            if all(selector in zip(attribute_group, value_group) for selector in zip(rule[0], rule[1])):\n",
    "                                is_irrelevant = True\n",
    "                                break\n",
    "                    if not is_irrelevant:\n",
    "                        # Generate the rule and add it to the set of rules\n",
    "                        # The rule is encoded as the set of attributes to match, their values and the class\n",
    "                        self.rules.append((attribute_group, value_group, unique_classes[0]))\n",
    "                        # Remove the classified instances from the set of non-classified ones\n",
    "                        indices_not_classified = np.setdiff1d(indices_not_classified, indices_match, assume_unique=True)\n",
    "\n",
    "                # If we are in the last iteration, we are checking for the full antecedent\n",
    "                # If there is more than a single class, we have a contradiction in the data\n",
    "                # Let's choose the most probable class (or random if tie)\n",
    "                # There won't be irrelevant conditions here because it is the last iteration and we have\n",
    "                # non-classified instances!\n",
    "                elif n_conditions == n_attributes:\n",
    "                    # print(\"WARNING: There are contradictions in the training set\")\n",
    "                    unique_classes, counts = np.unique(classes, return_counts=True)\n",
    "                    self.rules.append((attribute_group, value_group, unique_classes[np.argmax(counts)]))\n",
    "                    # Remove the classified instances from the set of non-classified ones\n",
    "                    indices_not_classified = np.setdiff1d(indices_not_classified, indices_match, assume_unique=True)\n",
    "\n",
    "    # ######################### P R E D I C T #########################\n",
    "    def predict(self, x):\n",
    "        # Predict\n",
    "        y_pred = self.__predict(x)\n",
    "        # Predictions are integers, convert back to original values\n",
    "        y_pred = np.vectorize(self.labels_dict[-1].get)(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.__predict(x)\n",
    "        # Convert true class to integers (predictions already are)\n",
    "        y = np.vectorize(self.preproc_dict[-1].get)(y)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "    def __predict(self, x):\n",
    "        print('Predicting...')\n",
    "        x = self.__preproc_test_data(x)\n",
    "        y_pred = []\n",
    "        # For each instance\n",
    "        for instance in x:\n",
    "            classified = False\n",
    "            # For each rule\n",
    "            for attributes, values, tag in self.rules:\n",
    "                # Check if antecedent matches\n",
    "                if np.array_equal(instance[list(attributes)], values):\n",
    "                    y_pred.append(tag)\n",
    "                    classified = True\n",
    "                    break\n",
    "            # No rule matched, we apply the default rule --> Most probable class\n",
    "            if not classified:\n",
    "                # print(\"WARNING: There are unseen combinations\")\n",
    "                y_pred.append(self.most_probable_class)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    # ################ M E T R I C S   A N D   R U L E S ################\n",
    "    def compute_metrics(self, x, y):\n",
    "        # Use the already computed (training) bins and integer conversions\n",
    "        x = self.__preproc_test_data(x)\n",
    "        y = np.vectorize(self.preproc_dict[-1].get)(y)\n",
    "        return self.__compute_metrics(x, y)\n",
    "\n",
    "    def __compute_metrics(self, x, y):\n",
    "        n_examples = x.shape[0]\n",
    "\n",
    "        # Store (Coverage, Precision) for each rule\n",
    "        metrics = []\n",
    "        overall_coverage = []\n",
    "        overall_precision = []\n",
    "        for attributes, values, tag in self.rules:\n",
    "            indices_match_condition = np.where((x[:, list(attributes)] == values).all(axis=1))[0]\n",
    "            coverage = len(indices_match_condition) / n_examples\n",
    "            indices_match_rule = np.where(y[indices_match_condition] == tag)[0]\n",
    "            precision = len(indices_match_rule) / len(indices_match_condition)\n",
    "            overall_precision.append(precision)\n",
    "            overall_coverage.append(coverage)\n",
    "            metrics.append((coverage, precision))\n",
    "        # Add overall metrics\n",
    "        metrics.append((sum(overall_coverage), sum(overall_precision) / len(overall_precision)))\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def __print_rules(self, metrics=None):\n",
    "        # Set default attribute names if not present in dataset\n",
    "        if self.attribute_names is None:\n",
    "            attribute_names = [f\"Attribute_{i + 1}\" for i in range(self.n_attributes)]\n",
    "            attribute_names.append(\"Class\")\n",
    "            self.attribute_names = attribute_names\n",
    "\n",
    "        rule_strings = []\n",
    "        for i in range(len(self.rules)):\n",
    "            attributes, values, tag = self.rules[i]\n",
    "            rule = f\"Rule {i + 1:3}.  IF \" \\\n",
    "                   f\"{' AND '.join([f'{self.attribute_names[attributes[j]]} IS {self.labels_dict[attributes[j]][values[j]]}' for j in range(len(attributes))])}\" \\\n",
    "                   f\" THEN {self.attribute_names[-1]} IS {self.labels_dict[-1][tag]}\"\n",
    "\n",
    "            if metrics:\n",
    "                rule = f\"{rule:100}    Coverage = {100 * metrics[i][0]:5.2f}%    Precision = \" \\\n",
    "                       f\"{100 * metrics[i][1]:5.2f}%\"\n",
    "            rule_strings.append(rule)\n",
    "        if metrics:\n",
    "            overall = f\"Overall Coverage = {100 * metrics[-1][0]:5.2f}%     Overall Precision = {100 * metrics[-1][1]:5.2f}%\"\n",
    "            rule_strings.append(overall)\n",
    "        return '\\n'.join(rule_strings)\n",
    "\n",
    "    # ############### P R E P R O C E S S I N G ###############\n",
    "    def __preproc_train_data(self, x, y):\n",
    "        # Set attribute names for pretty printing\n",
    "        if self.contains_header:\n",
    "            names_x = x.columns.values.tolist()\n",
    "            name_y = y.columns.values.tolist()\n",
    "            self.attribute_names = names_x + name_y\n",
    "\n",
    "        column_types = x.dtypes\n",
    "        # Missing Values\n",
    "        for attribute, dtype in zip(x, column_types):\n",
    "            # We take the mean for floats\n",
    "            if np.issubdtype(dtype, np.floating):\n",
    "                x.loc[:, attribute].fillna(x[attribute].mean(), inplace=True)\n",
    "            # We take the mode for categoricals (including integers)\n",
    "            else:\n",
    "                # Intermediate conversion into '?' (to join different representations for mv)\n",
    "                x.loc[:, attribute].fillna('?', inplace=True)\n",
    "                uniques, counts = np.unique(x[attribute], return_counts=True)\n",
    "                mode = uniques[np.argmax(counts)]\n",
    "                if mode == '?':\n",
    "                    mode = uniques[np.argsort(counts)[-2]]\n",
    "                x.loc[x[attribute] == '?', attribute] = mode\n",
    "\n",
    "        # Discretize Numeric attributes\n",
    "        # Store exact discretization bins for test data\n",
    "        if self.number_bins != 0:\n",
    "            to_discretize = np.number if self.discretize_ints else np.floating\n",
    "            for attribute, dtype in zip(x, column_types):\n",
    "                if np.issubdtype(dtype, to_discretize):\n",
    "                    if self.discretize_mode == 'equal':\n",
    "                        x[attribute], bins = pd.cut(x[attribute], bins=self.number_bins, retbins=True)\n",
    "                        self.bins.append((attribute, bins, np.unique(x[attribute])))\n",
    "                    elif self.discretize_mode == 'freq':\n",
    "                        x[attribute], bins = pd.qcut(x[attribute], q=self.number_bins, retbins=True)\n",
    "                        self.bins.append((attribute, bins, np.unique(x[attribute])))\n",
    "                    else:\n",
    "                        raise ValueError(\"Wrong discretize_mode\")\n",
    "\n",
    "        # Move everything to integer, so numpy works faster\n",
    "        x = x.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        data = np.concatenate((x, y), axis=1)\n",
    "        _, n_cols = data.shape\n",
    "\n",
    "        # Store conversion from original values to integers for pretty printing\n",
    "        inv_conversions = []\n",
    "        conversions = []\n",
    "        for i in range(n_cols):\n",
    "            col = data[:, i]\n",
    "            uniques = np.unique(col).tolist()\n",
    "            d = defaultdict(lambda: -1, zip(uniques, range(len(uniques))))\n",
    "            d_inv = dict(zip(range(len(uniques)), uniques))\n",
    "            data[:, i] = np.vectorize(d.get)(col)\n",
    "            conversions.append(d)\n",
    "            inv_conversions.append(d_inv)\n",
    "\n",
    "        # Preprocessing Dictionary\n",
    "        # Contains all the conversions from original values to integers --> To be used when preprocessing test data\n",
    "        self.preproc_dict = conversions\n",
    "        # Labels Dictionary\n",
    "        # Contains all the conversions from integers to original values --> To be used when pretty printing\n",
    "        self.labels_dict = inv_conversions\n",
    "\n",
    "        return data[:, :-1].astype(np.uint8), data[:, -1].astype(np.uint8)\n",
    "\n",
    "    def __preproc_test_data(self, x):\n",
    "        # Preprocess only attributes (not class)\n",
    "        # Use the same exact steps than in training\n",
    "        #   MV\n",
    "        #   Discretization using same bins\n",
    "        #   Conversion to integers using same mapping\n",
    "\n",
    "        column_types = x.dtypes\n",
    "        # Missing Values\n",
    "        for attribute, dtype in zip(x, column_types):\n",
    "            # We take the mean for floats\n",
    "            if np.issubdtype(dtype, np.floating):\n",
    "                x.loc[:, attribute].fillna(x[attribute].mean(), inplace=True)\n",
    "            # We take the mode for categoricals (including integers)\n",
    "            else:\n",
    "                # Intermediate conversion into '?' (to join different representations for mv)\n",
    "                x.loc[:, attribute].fillna('?', inplace=True)\n",
    "                uniques, counts = np.unique(x[attribute], return_counts=True)\n",
    "                mode = uniques[np.argmax(counts)]\n",
    "                if mode == '?':\n",
    "                    mode = uniques[np.argsort(counts)[-2]]\n",
    "                x.loc[x[attribute] == '?', attribute] = mode\n",
    "\n",
    "        # Discretize Numeric attributes using training bins\n",
    "        for attribute, bins, labels in self.bins:\n",
    "            if len(labels) + 1 == len(bins):\n",
    "                x[attribute] = pd.cut(x[attribute], bins=bins, labels=labels)\n",
    "            else:\n",
    "                x[attribute] = pd.cut(x[attribute], bins=bins)\n",
    "\n",
    "        # Move everything to integer, so numpy works faster\n",
    "        data = x.to_numpy()\n",
    "        _, n_cols = data.shape\n",
    "\n",
    "        # Use original-integer training mapping\n",
    "        for i in range(n_cols):\n",
    "            col = data[:, i]\n",
    "            data[:, i] = np.vectorize(self.__my_vectorized_mapping)(i, col)\n",
    "\n",
    "        return data.astype(np.uint8)\n",
    "\n",
    "    def __my_vectorized_mapping(self, i, x):\n",
    "        return self.preproc_dict[i][x]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from csv and storing in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change file name here to run different datasets\n",
    "df = pd.read_csv('datasets/tic-tac-toe.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X, Y split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df.iloc[:, :-1]\n",
    "y_df = df.iloc[:, -1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Original RULES algorithm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting in training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rules object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = RULES(contains_header=True, number_bins=3, discretize_mode='equal', discretize_ints=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Original RULES...\n",
      "\n",
      "Induced rules:\n",
      "Rule   1.  IF att1 IS b AND att2 IS b AND att3 IS b THEN Class IS positive                              Coverage =  0.78%    Precision = 100.00%\n",
      "Rule   2.  IF att1 IS b AND att2 IS o AND att3 IS o THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule   3.  IF att1 IS o AND att2 IS o AND att3 IS o THEN Class IS negative                              Coverage =  3.39%    Precision = 100.00%\n",
      "Rule   4.  IF att1 IS x AND att2 IS x AND att3 IS x THEN Class IS positive                              Coverage =  8.49%    Precision = 100.00%\n",
      "Rule   5.  IF att1 IS b AND att2 IS o AND att4 IS o THEN Class IS positive                              Coverage =  1.96%    Precision = 100.00%\n",
      "Rule   6.  IF att1 IS b AND att2 IS o AND att5 IS b THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule   7.  IF att1 IS b AND att2 IS o AND att5 IS x THEN Class IS positive                              Coverage =  3.66%    Precision = 100.00%\n",
      "Rule   8.  IF att1 IS b AND att2 IS o AND att7 IS o THEN Class IS positive                              Coverage =  1.04%    Precision = 100.00%\n",
      "Rule   9.  IF att1 IS b AND att2 IS o AND att8 IS b THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  10.  IF att1 IS b AND att2 IS o AND att8 IS x THEN Class IS positive                              Coverage =  2.61%    Precision = 100.00%\n",
      "Rule  11.  IF att1 IS o AND att2 IS b AND att8 IS o THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  12.  IF att1 IS b AND att2 IS o AND att9 IS o THEN Class IS positive                              Coverage =  1.31%    Precision = 100.00%\n",
      "Rule  13.  IF att1 IS b AND att3 IS o AND att4 IS o THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  14.  IF att1 IS b AND att4 IS o AND att5 IS b THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule  15.  IF att1 IS b AND att4 IS o AND att6 IS b THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule  16.  IF att1 IS b AND att4 IS b AND att7 IS b THEN Class IS positive                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  17.  IF att1 IS o AND att4 IS o AND att7 IS o THEN Class IS negative                              Coverage =  4.05%    Precision = 100.00%\n",
      "Rule  18.  IF att1 IS x AND att4 IS x AND att7 IS x THEN Class IS positive                              Coverage =  7.83%    Precision = 100.00%\n",
      "Rule  19.  IF att1 IS b AND att5 IS b AND att6 IS b THEN Class IS positive                              Coverage =  0.39%    Precision = 100.00%\n",
      "Rule  20.  IF att1 IS b AND att5 IS b AND att9 IS o THEN Class IS negative                              Coverage =  0.91%    Precision = 100.00%\n",
      "Rule  21.  IF att1 IS b AND att5 IS b AND att9 IS x THEN Class IS positive                              Coverage =  1.96%    Precision = 100.00%\n",
      "Rule  22.  IF att1 IS b AND att5 IS o AND att9 IS b THEN Class IS negative                              Coverage =  1.31%    Precision = 100.00%\n",
      "Rule  23.  IF att1 IS b AND att5 IS o AND att9 IS o THEN Class IS negative                              Coverage =  0.26%    Precision = 100.00%\n",
      "Rule  24.  IF att1 IS b AND att5 IS x AND att9 IS b THEN Class IS positive                              Coverage =  2.87%    Precision = 100.00%\n",
      "Rule  25.  IF att1 IS b AND att5 IS x AND att9 IS x THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  26.  IF att1 IS o AND att5 IS b AND att9 IS b THEN Class IS negative                              Coverage =  0.91%    Precision = 100.00%\n",
      "Rule  27.  IF att1 IS o AND att5 IS b AND att9 IS o THEN Class IS negative                              Coverage =  0.39%    Precision = 100.00%\n",
      "Rule  28.  IF att1 IS o AND att5 IS o AND att9 IS b THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  29.  IF att1 IS o AND att5 IS o AND att9 IS o THEN Class IS negative                              Coverage =  5.87%    Precision = 100.00%\n",
      "Rule  30.  IF att1 IS x AND att5 IS b AND att9 IS b THEN Class IS positive                              Coverage =  2.35%    Precision = 100.00%\n",
      "Rule  31.  IF att1 IS x AND att5 IS b AND att9 IS x THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  32.  IF att1 IS x AND att5 IS x AND att9 IS b THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule  33.  IF att1 IS x AND att5 IS x AND att9 IS x THEN Class IS positive                              Coverage = 10.18%    Precision = 100.00%\n",
      "Rule  34.  IF att1 IS b AND att6 IS b AND att7 IS b THEN Class IS positive                              Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  35.  IF att2 IS o AND att3 IS b AND att5 IS b THEN Class IS positive                              Coverage =  1.44%    Precision = 100.00%\n",
      "Rule  36.  IF att2 IS o AND att3 IS b AND att6 IS o THEN Class IS positive                              Coverage =  2.48%    Precision = 100.00%\n",
      "Rule  37.  IF att2 IS o AND att3 IS b AND att8 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  38.  IF att2 IS o AND att4 IS o AND att5 IS b THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule  39.  IF att2 IS o AND att4 IS o AND att6 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  40.  IF att2 IS o AND att5 IS b AND att6 IS o THEN Class IS positive                              Coverage =  1.44%    Precision = 100.00%\n",
      "Rule  41.  IF att2 IS b AND att5 IS b AND att8 IS b THEN Class IS positive                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  42.  IF att2 IS o AND att5 IS o AND att8 IS o THEN Class IS negative                              Coverage =  3.79%    Precision = 100.00%\n",
      "Rule  43.  IF att2 IS x AND att5 IS x AND att8 IS x THEN Class IS positive                              Coverage =  7.57%    Precision = 100.00%\n",
      "Rule  44.  IF att2 IS b AND att7 IS b AND att8 IS o THEN Class IS positive                              Coverage =  2.35%    Precision = 100.00%\n",
      "Rule  45.  IF att2 IS b AND att8 IS o AND att9 IS b THEN Class IS positive                              Coverage =  1.83%    Precision = 100.00%\n",
      "Rule  46.  IF att3 IS b AND att4 IS b AND att6 IS o THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  47.  IF att3 IS b AND att5 IS b AND att6 IS o THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  48.  IF att3 IS b AND att5 IS b AND att7 IS o THEN Class IS negative                              Coverage =  0.78%    Precision = 100.00%\n",
      "Rule  49.  IF att3 IS b AND att5 IS b AND att7 IS x THEN Class IS positive                              Coverage =  2.35%    Precision = 100.00%\n",
      "Rule  50.  IF att3 IS b AND att5 IS o AND att7 IS b THEN Class IS negative                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  51.  IF att3 IS b AND att5 IS o AND att7 IS o THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  52.  IF att3 IS b AND att5 IS x AND att7 IS b THEN Class IS positive                              Coverage =  3.39%    Precision = 100.00%\n",
      "Rule  53.  IF att3 IS b AND att5 IS x AND att7 IS x THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  54.  IF att3 IS o AND att5 IS b AND att7 IS b THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  55.  IF att3 IS o AND att5 IS b AND att7 IS o THEN Class IS negative                              Coverage =  0.52%    Precision = 100.00%\n",
      "Rule  56.  IF att3 IS o AND att5 IS o AND att7 IS b THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  57.  IF att3 IS o AND att5 IS o AND att7 IS o THEN Class IS negative                              Coverage =  5.09%    Precision = 100.00%\n",
      "Rule  58.  IF att3 IS x AND att5 IS b AND att7 IS b THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule  59.  IF att3 IS x AND att5 IS b AND att7 IS x THEN Class IS positive                              Coverage =  0.91%    Precision = 100.00%\n",
      "Rule  60.  IF att3 IS x AND att5 IS x AND att7 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  61.  IF att3 IS x AND att5 IS x AND att7 IS x THEN Class IS positive                              Coverage =  9.01%    Precision = 100.00%\n",
      "Rule  62.  IF att3 IS b AND att6 IS b AND att9 IS b THEN Class IS positive                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  63.  IF att3 IS o AND att6 IS o AND att9 IS o THEN Class IS negative                              Coverage =  3.79%    Precision = 100.00%\n",
      "Rule  64.  IF att3 IS x AND att6 IS x AND att9 IS x THEN Class IS positive                              Coverage =  7.57%    Precision = 100.00%\n",
      "Rule  65.  IF att4 IS b AND att5 IS b AND att6 IS b THEN Class IS positive                              Coverage =  0.52%    Precision = 100.00%\n",
      "Rule  66.  IF att4 IS o AND att5 IS o AND att6 IS o THEN Class IS negative                              Coverage =  4.31%    Precision = 100.00%\n",
      "Rule  67.  IF att4 IS x AND att5 IS x AND att6 IS x THEN Class IS positive                              Coverage =  8.36%    Precision = 100.00%\n",
      "Rule  68.  IF att4 IS o AND att5 IS b AND att7 IS b THEN Class IS positive                              Coverage =  1.04%    Precision = 100.00%\n",
      "Rule  69.  IF att4 IS o AND att5 IS b AND att8 IS o THEN Class IS positive                              Coverage =  1.44%    Precision = 100.00%\n",
      "Rule  70.  IF att4 IS o AND att6 IS b AND att7 IS b THEN Class IS positive                              Coverage =  1.83%    Precision = 100.00%\n",
      "Rule  71.  IF att4 IS b AND att6 IS o AND att9 IS b THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  72.  IF att4 IS o AND att7 IS b AND att8 IS o THEN Class IS positive                              Coverage =  2.61%    Precision = 100.00%\n",
      "Rule  73.  IF att5 IS b AND att6 IS b AND att7 IS b THEN Class IS positive                              Coverage =  0.26%    Precision = 100.00%\n",
      "Rule  74.  IF att5 IS b AND att6 IS o AND att8 IS o THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  75.  IF att5 IS b AND att6 IS o AND att9 IS b THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  76.  IF att5 IS x AND att6 IS o AND att9 IS b THEN Class IS positive                              Coverage =  3.39%    Precision = 100.00%\n",
      "Rule  77.  IF att5 IS b AND att7 IS b AND att8 IS o THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule  78.  IF att5 IS b AND att7 IS b AND att8 IS x THEN Class IS negative                              Coverage =  0.52%    Precision = 100.00%\n",
      "Rule  79.  IF att5 IS b AND att8 IS o AND att9 IS b THEN Class IS positive                              Coverage =  1.44%    Precision = 100.00%\n",
      "Rule  80.  IF att6 IS o AND att8 IS o AND att9 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  81.  IF att7 IS b AND att8 IS b AND att9 IS b THEN Class IS positive                              Coverage =  0.78%    Precision = 100.00%\n",
      "Rule  82.  IF att7 IS o AND att8 IS o AND att9 IS o THEN Class IS negative                              Coverage =  4.05%    Precision = 100.00%\n",
      "Rule  83.  IF att7 IS x AND att8 IS x AND att9 IS x THEN Class IS positive                              Coverage =  7.83%    Precision = 100.00%\n",
      "Rule  84.  IF att1 IS o AND att2 IS x AND att3 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.39%    Precision = 100.00%\n",
      "Rule  85.  IF att1 IS o AND att2 IS o AND att4 IS o AND att5 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  86.  IF att1 IS x AND att2 IS x AND att4 IS x AND att6 IS x THEN Class IS negative                Coverage =  0.52%    Precision = 100.00%\n",
      "Rule  87.  IF att1 IS o AND att2 IS o AND att4 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  88.  IF att1 IS o AND att2 IS o AND att4 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  89.  IF att1 IS o AND att2 IS o AND att5 IS o AND att6 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  90.  IF att1 IS o AND att2 IS o AND att5 IS o AND att7 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  91.  IF att1 IS o AND att2 IS o AND att5 IS x AND att9 IS o THEN Class IS positive                Coverage =  1.31%    Precision = 100.00%\n",
      "Rule  92.  IF att1 IS o AND att2 IS o AND att6 IS o AND att7 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  93.  IF att1 IS o AND att2 IS o AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  94.  IF att1 IS o AND att2 IS o AND att6 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  95.  IF att1 IS o AND att2 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  96.  IF att1 IS x AND att2 IS x AND att7 IS x AND att8 IS x THEN Class IS negative                Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  97.  IF att1 IS o AND att2 IS o AND att7 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  98.  IF att1 IS o AND att2 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  99.  IF att1 IS o AND att3 IS o AND att4 IS o AND att6 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 100.  IF att1 IS x AND att3 IS x AND att4 IS x AND att6 IS x THEN Class IS negative                Coverage =  0.52%    Precision = 100.00%\n",
      "Rule 101.  IF att1 IS o AND att3 IS o AND att4 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 102.  IF att1 IS o AND att3 IS o AND att5 IS o AND att6 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 103.  IF att1 IS o AND att3 IS o AND att6 IS o AND att7 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 104.  IF att1 IS o AND att3 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 105.  IF att1 IS o AND att3 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 106.  IF att1 IS x AND att4 IS o AND att5 IS x AND att6 IS o THEN Class IS positive                Coverage =  2.74%    Precision = 100.00%\n",
      "Rule 107.  IF att1 IS o AND att4 IS o AND att5 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 108.  IF att1 IS o AND att4 IS o AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 109.  IF att1 IS x AND att4 IS x AND att6 IS x AND att8 IS x THEN Class IS negative                Coverage =  0.26%    Precision = 100.00%\n",
      "Rule 110.  IF att1 IS o AND att4 IS o AND att6 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 111.  IF att1 IS o AND att4 IS o AND att7 IS x AND att8 IS o THEN Class IS positive                Coverage =  0.78%    Precision = 100.00%\n",
      "Rule 112.  IF att1 IS o AND att4 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 113.  IF att1 IS o AND att5 IS o AND att6 IS o AND att7 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 114.  IF att1 IS o AND att5 IS o AND att6 IS o AND att8 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 115.  IF att1 IS o AND att5 IS x AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.65%    Precision = 100.00%\n",
      "Rule 116.  IF att1 IS o AND att5 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 117.  IF att1 IS o AND att5 IS x AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  1.17%    Precision = 100.00%\n",
      "Rule 118.  IF att1 IS o AND att6 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 119.  IF att2 IS o AND att3 IS o AND att4 IS o AND att5 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 120.  IF att2 IS o AND att3 IS o AND att4 IS o AND att6 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 121.  IF att2 IS o AND att3 IS o AND att4 IS o AND att7 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 122.  IF att2 IS o AND att3 IS o AND att4 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 123.  IF att2 IS o AND att3 IS o AND att4 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 124.  IF att2 IS o AND att3 IS o AND att5 IS o AND att6 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 125.  IF att2 IS x AND att3 IS o AND att5 IS o AND att6 IS x THEN Class IS negative                Coverage =  1.96%    Precision = 100.00%\n",
      "Rule 126.  IF att2 IS x AND att3 IS o AND att5 IS o AND att9 IS o THEN Class IS negative                Coverage =  1.17%    Precision = 100.00%\n",
      "Rule 127.  IF att2 IS o AND att3 IS o AND att6 IS o AND att7 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 128.  IF att2 IS o AND att3 IS o AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 129.  IF att2 IS o AND att3 IS x AND att6 IS o AND att9 IS o THEN Class IS positive                Coverage =  1.04%    Precision = 100.00%\n",
      "Rule 130.  IF att2 IS o AND att3 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 131.  IF att2 IS x AND att3 IS x AND att8 IS x AND att9 IS x THEN Class IS negative                Coverage =  0.65%    Precision = 100.00%\n",
      "Rule 132.  IF att2 IS o AND att4 IS o AND att5 IS x AND att6 IS o THEN Class IS positive                Coverage =  0.91%    Precision = 100.00%\n",
      "Rule 133.  IF att2 IS o AND att4 IS o AND att5 IS o AND att7 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 134.  IF att2 IS o AND att4 IS o AND att5 IS x AND att8 IS o THEN Class IS positive                Coverage =  1.44%    Precision = 100.00%\n",
      "Rule 135.  IF att2 IS o AND att4 IS o AND att5 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 136.  IF att2 IS x AND att4 IS x AND att5 IS x AND att9 IS x THEN Class IS positive                Coverage =  0.39%    Precision = 100.00%\n",
      "Rule 137.  IF att2 IS o AND att4 IS o AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 138.  IF att2 IS x AND att4 IS x AND att6 IS x AND att8 IS x THEN Class IS negative                Coverage =  0.52%    Precision = 100.00%\n",
      "Rule 139.  IF att2 IS o AND att4 IS o AND att6 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 140.  IF att2 IS o AND att4 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 141.  IF att2 IS o AND att4 IS o AND att7 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 142.  IF att2 IS o AND att4 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 143.  IF att2 IS o AND att5 IS o AND att6 IS o AND att7 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 144.  IF att2 IS o AND att5 IS x AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  1.31%    Precision = 100.00%\n",
      "Rule 145.  IF att2 IS o AND att5 IS o AND att6 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 146.  IF att2 IS x AND att5 IS x AND att7 IS x AND att9 IS x THEN Class IS positive                Coverage =  0.39%    Precision = 100.00%\n",
      "Rule 147.  IF att2 IS o AND att6 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 148.  IF att2 IS o AND att6 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 149.  IF att2 IS x AND att6 IS x AND att8 IS x AND att9 IS x THEN Class IS negative                Coverage =  0.26%    Precision = 100.00%\n",
      "Rule 150.  IF att3 IS o AND att4 IS o AND att5 IS x AND att6 IS o THEN Class IS positive                Coverage =  1.04%    Precision = 100.00%\n",
      "Rule 151.  IF att3 IS o AND att4 IS o AND att5 IS o AND att8 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 152.  IF att3 IS o AND att4 IS o AND att5 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 153.  IF att3 IS o AND att4 IS o AND att6 IS o AND att7 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 154.  IF att3 IS o AND att4 IS o AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 155.  IF att3 IS o AND att4 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 156.  IF att3 IS o AND att4 IS o AND att7 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 157.  IF att3 IS o AND att4 IS o AND att8 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 158.  IF att3 IS o AND att5 IS o AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 159.  IF att3 IS o AND att5 IS o AND att6 IS x AND att8 IS o THEN Class IS negative                Coverage =  0.91%    Precision = 100.00%\n",
      "Rule 160.  IF att3 IS o AND att5 IS o AND att8 IS x AND att9 IS o THEN Class IS negative                Coverage =  1.31%    Precision = 100.00%\n",
      "Rule 161.  IF att3 IS x AND att5 IS x AND att8 IS x AND att9 IS x THEN Class IS negative                Coverage =  0.26%    Precision = 100.00%\n",
      "Rule 162.  IF att3 IS o AND att6 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 163.  IF att3 IS o AND att6 IS o AND att8 IS o AND att9 IS x THEN Class IS positive                Coverage =  1.04%    Precision = 100.00%\n",
      "Rule 164.  IF att4 IS o AND att5 IS x AND att6 IS o AND att7 IS x THEN Class IS positive                Coverage =  2.87%    Precision = 100.00%\n",
      "Rule 165.  IF att4 IS o AND att5 IS x AND att6 IS o AND att8 IS o THEN Class IS positive                Coverage =  1.17%    Precision = 100.00%\n",
      "Rule 166.  IF att4 IS o AND att5 IS x AND att6 IS o AND att9 IS o THEN Class IS positive                Coverage =  1.31%    Precision = 100.00%\n",
      "Rule 167.  IF att4 IS o AND att5 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 168.  IF att4 IS o AND att5 IS o AND att7 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 169.  IF att4 IS o AND att5 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 170.  IF att4 IS o AND att6 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 171.  IF att4 IS x AND att6 IS x AND att7 IS x AND att8 IS x THEN Class IS negative                Coverage =  0.39%    Precision = 100.00%\n",
      "Rule 172.  IF att4 IS o AND att6 IS o AND att7 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 173.  IF att4 IS x AND att6 IS x AND att7 IS x AND att9 IS x THEN Class IS negative                Coverage =  0.26%    Precision = 100.00%\n",
      "Rule 174.  IF att4 IS o AND att6 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 175.  IF att5 IS o AND att6 IS o AND att7 IS o AND att8 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 176.  IF att5 IS o AND att6 IS o AND att7 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 177.  IF att5 IS o AND att6 IS o AND att8 IS o AND att9 IS o THEN Class IS positive                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule 178.  IF att5 IS x AND att6 IS o AND att8 IS o AND att9 IS x THEN Class IS positive                Coverage =  2.74%    Precision = 100.00%\n",
      "Rule 179.  IF att6 IS o AND att7 IS o AND att8 IS o AND att9 IS x THEN Class IS positive                Coverage =  0.91%    Precision = 100.00%\n",
      "Overall Coverage = 243.34%     Overall Precision = 100.00%\n",
      "\n",
      "Time to preprocess data = 0.01s\n",
      "Time to fit data = 0.19s\n",
      "Time to comput metrics = 0.00s\n",
      "Time to print rules = 0.00s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rules.fit(x_train, y_train, method='Original', show_rules=True, show_time=True, show_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Accuracy on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Test Accuracy = 92.71%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.76      0.87        59\n",
      "    positive       0.90      1.00      0.95       133\n",
      "\n",
      "    accuracy                           0.93       192\n",
      "   macro avg       0.95      0.88      0.91       192\n",
      "weighted avg       0.93      0.93      0.92       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rules.predict(x_test)\n",
    "print(f'Test Accuracy = {100 * accuracy_score(y_test.to_numpy(), y_pred):.2f}%')\n",
    "print(classification_report(y_test.to_numpy(), y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RRULES algorithm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting in training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rules object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = RULES(contains_header=True, number_bins=3, discretize_mode='equal', discretize_ints=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RRULES...\n",
      "\n",
      "Induced rules:\n",
      "Rule   1.  IF att1 IS b AND att2 IS b AND att3 IS b THEN Class IS positive                              Coverage =  0.78%    Precision = 100.00%\n",
      "Rule   2.  IF att1 IS b AND att2 IS o AND att3 IS o THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule   3.  IF att1 IS o AND att2 IS o AND att3 IS o THEN Class IS negative                              Coverage =  3.39%    Precision = 100.00%\n",
      "Rule   4.  IF att1 IS x AND att2 IS x AND att3 IS x THEN Class IS positive                              Coverage =  8.49%    Precision = 100.00%\n",
      "Rule   5.  IF att1 IS b AND att2 IS o AND att4 IS o THEN Class IS positive                              Coverage =  1.96%    Precision = 100.00%\n",
      "Rule   6.  IF att1 IS b AND att2 IS o AND att5 IS b THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule   7.  IF att1 IS b AND att2 IS o AND att5 IS x THEN Class IS positive                              Coverage =  3.66%    Precision = 100.00%\n",
      "Rule   8.  IF att1 IS b AND att2 IS o AND att7 IS o THEN Class IS positive                              Coverage =  1.04%    Precision = 100.00%\n",
      "Rule   9.  IF att1 IS b AND att2 IS o AND att8 IS b THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  10.  IF att1 IS b AND att2 IS o AND att8 IS x THEN Class IS positive                              Coverage =  2.61%    Precision = 100.00%\n",
      "Rule  11.  IF att1 IS o AND att2 IS b AND att8 IS o THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  12.  IF att1 IS b AND att3 IS o AND att4 IS o THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  13.  IF att1 IS b AND att4 IS o AND att5 IS b THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule  14.  IF att1 IS b AND att4 IS o AND att6 IS b THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule  15.  IF att1 IS b AND att4 IS b AND att7 IS b THEN Class IS positive                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  16.  IF att1 IS o AND att4 IS o AND att7 IS o THEN Class IS negative                              Coverage =  4.05%    Precision = 100.00%\n",
      "Rule  17.  IF att1 IS x AND att4 IS x AND att7 IS x THEN Class IS positive                              Coverage =  7.83%    Precision = 100.00%\n",
      "Rule  18.  IF att1 IS b AND att5 IS b AND att9 IS o THEN Class IS negative                              Coverage =  0.91%    Precision = 100.00%\n",
      "Rule  19.  IF att1 IS b AND att5 IS o AND att9 IS b THEN Class IS negative                              Coverage =  1.31%    Precision = 100.00%\n",
      "Rule  20.  IF att1 IS b AND att5 IS o AND att9 IS o THEN Class IS negative                              Coverage =  0.26%    Precision = 100.00%\n",
      "Rule  21.  IF att1 IS b AND att5 IS x AND att9 IS b THEN Class IS positive                              Coverage =  2.87%    Precision = 100.00%\n",
      "Rule  22.  IF att1 IS b AND att5 IS x AND att9 IS x THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  23.  IF att1 IS o AND att5 IS b AND att9 IS o THEN Class IS negative                              Coverage =  0.39%    Precision = 100.00%\n",
      "Rule  24.  IF att1 IS o AND att5 IS o AND att9 IS b THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  25.  IF att1 IS o AND att5 IS o AND att9 IS o THEN Class IS negative                              Coverage =  5.87%    Precision = 100.00%\n",
      "Rule  26.  IF att1 IS x AND att5 IS b AND att9 IS x THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  27.  IF att1 IS x AND att5 IS x AND att9 IS b THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule  28.  IF att1 IS x AND att5 IS x AND att9 IS x THEN Class IS positive                              Coverage = 10.18%    Precision = 100.00%\n",
      "Rule  29.  IF att2 IS o AND att3 IS b AND att5 IS b THEN Class IS positive                              Coverage =  1.44%    Precision = 100.00%\n",
      "Rule  30.  IF att2 IS o AND att3 IS b AND att6 IS o THEN Class IS positive                              Coverage =  2.48%    Precision = 100.00%\n",
      "Rule  31.  IF att2 IS o AND att3 IS b AND att8 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  32.  IF att2 IS o AND att4 IS o AND att5 IS b THEN Class IS positive                              Coverage =  1.17%    Precision = 100.00%\n",
      "Rule  33.  IF att2 IS o AND att4 IS o AND att6 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  34.  IF att2 IS b AND att5 IS b AND att8 IS b THEN Class IS positive                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  35.  IF att2 IS o AND att5 IS o AND att8 IS o THEN Class IS negative                              Coverage =  3.79%    Precision = 100.00%\n",
      "Rule  36.  IF att2 IS x AND att5 IS x AND att8 IS x THEN Class IS positive                              Coverage =  7.57%    Precision = 100.00%\n",
      "Rule  37.  IF att2 IS b AND att7 IS b AND att8 IS o THEN Class IS positive                              Coverage =  2.35%    Precision = 100.00%\n",
      "Rule  38.  IF att3 IS b AND att4 IS b AND att6 IS o THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  39.  IF att3 IS b AND att5 IS b AND att6 IS o THEN Class IS positive                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  40.  IF att3 IS b AND att5 IS b AND att7 IS o THEN Class IS negative                              Coverage =  0.78%    Precision = 100.00%\n",
      "Rule  41.  IF att3 IS b AND att5 IS o AND att7 IS b THEN Class IS negative                              Coverage =  1.57%    Precision = 100.00%\n",
      "Rule  42.  IF att3 IS b AND att5 IS o AND att7 IS o THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  43.  IF att3 IS b AND att5 IS x AND att7 IS b THEN Class IS positive                              Coverage =  3.39%    Precision = 100.00%\n",
      "Rule  44.  IF att3 IS b AND att5 IS x AND att7 IS x THEN Class IS positive                              Coverage =  2.09%    Precision = 100.00%\n",
      "Rule  45.  IF att3 IS o AND att5 IS b AND att7 IS b THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  46.  IF att3 IS o AND att5 IS b AND att7 IS o THEN Class IS negative                              Coverage =  0.52%    Precision = 100.00%\n",
      "Rule  47.  IF att3 IS o AND att5 IS o AND att7 IS b THEN Class IS negative                              Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  48.  IF att3 IS o AND att5 IS o AND att7 IS o THEN Class IS negative                              Coverage =  5.09%    Precision = 100.00%\n",
      "Rule  49.  IF att3 IS x AND att5 IS b AND att7 IS b THEN Class IS positive                              Coverage =  1.70%    Precision = 100.00%\n",
      "Rule  50.  IF att3 IS x AND att5 IS x AND att7 IS b THEN Class IS positive                              Coverage =  2.22%    Precision = 100.00%\n",
      "Rule  51.  IF att3 IS x AND att5 IS x AND att7 IS x THEN Class IS positive                              Coverage =  9.01%    Precision = 100.00%\n",
      "Rule  52.  IF att3 IS o AND att6 IS o AND att9 IS o THEN Class IS negative                              Coverage =  3.79%    Precision = 100.00%\n",
      "Rule  53.  IF att3 IS x AND att6 IS x AND att9 IS x THEN Class IS positive                              Coverage =  7.57%    Precision = 100.00%\n",
      "Rule  54.  IF att4 IS b AND att5 IS b AND att6 IS b THEN Class IS positive                              Coverage =  0.52%    Precision = 100.00%\n",
      "Rule  55.  IF att4 IS o AND att5 IS o AND att6 IS o THEN Class IS negative                              Coverage =  4.31%    Precision = 100.00%\n",
      "Rule  56.  IF att4 IS x AND att5 IS x AND att6 IS x THEN Class IS positive                              Coverage =  8.36%    Precision = 100.00%\n",
      "Rule  57.  IF att7 IS o AND att8 IS o AND att9 IS o THEN Class IS negative                              Coverage =  4.05%    Precision = 100.00%\n",
      "Rule  58.  IF att7 IS x AND att8 IS x AND att9 IS x THEN Class IS positive                              Coverage =  7.83%    Precision = 100.00%\n",
      "Rule  59.  IF att1 IS o AND att2 IS o AND att6 IS o AND att7 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  60.  IF att1 IS x AND att2 IS x AND att7 IS x AND att8 IS x THEN Class IS negative                Coverage =  0.65%    Precision = 100.00%\n",
      "Rule  61.  IF att1 IS o AND att5 IS o AND att6 IS o AND att7 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  62.  IF att1 IS o AND att5 IS o AND att6 IS o AND att8 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  63.  IF att2 IS o AND att3 IS o AND att4 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  64.  IF att2 IS x AND att3 IS o AND att5 IS o AND att6 IS x THEN Class IS negative                Coverage =  1.96%    Precision = 100.00%\n",
      "Rule  65.  IF att2 IS o AND att4 IS o AND att5 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  66.  IF att2 IS o AND att4 IS o AND att7 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  67.  IF att2 IS o AND att5 IS o AND att6 IS o AND att7 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Rule  68.  IF att3 IS o AND att4 IS o AND att8 IS o AND att9 IS o THEN Class IS negative                Coverage =  0.13%    Precision = 100.00%\n",
      "Overall Coverage = 169.32%     Overall Precision = 100.00%\n",
      "\n",
      "Time to preprocess data = 0.01s\n",
      "Time to fit data = 0.31s\n",
      "Time to comput metrics = 0.00s\n",
      "Time to print rules = 0.00s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rules.fit(x_train, y_train, method='RRULES', show_rules=True, show_time=True, show_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Test Accuracy = 94.27%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.81      0.90        59\n",
      "    positive       0.92      1.00      0.96       133\n",
      "\n",
      "    accuracy                           0.94       192\n",
      "   macro avg       0.96      0.91      0.93       192\n",
      "weighted avg       0.95      0.94      0.94       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rules.predict(x_test)\n",
    "print(f'Test Accuracy = {100 * accuracy_score(y_test.to_numpy(), y_pred):.2f}%')\n",
    "print(classification_report(y_test.to_numpy(), y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see from above example that RRULES perform better compared to original RULES algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
